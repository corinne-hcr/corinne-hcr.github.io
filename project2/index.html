<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>CS 180 Project 2: Fun with Filters and Frequencies</title>
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            background-color: #f5f5f5;
        }
        h1 {
            color: #003262;
            text-align: center;
            border-bottom: 3px solid #FDB515;
            padding-bottom: 10px;
        }
        h2 {
            color: #003262;
            margin-top: 40px;
            border-left: 5px solid #FDB515;
            padding-left: 15px;
        }
        h3 {
            color: #003262;
            margin-top: 30px;
        }
        h4 {
            color: #003262;
            margin-top: 25px;
        }
        .section {
            background: white;
            padding: 25px;
            margin: 20px 0;
            border-radius: 8px;
            box-shadow: 0 2px 5px rgba(0,0,0,0.1);
        }
        .image-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 20px;
            margin: 20px 0;
        }
        .image-container {
            text-align: center;
        }
        .image-container img {
            max-width: 100%;
            height: auto;
            border-radius: 5px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        .image-caption {
            margin-top: 8px;
            font-style: italic;
            color: #555;
        }
        .full-width-image {
            text-align: center;
            margin: 20px 0;
        }
        .full-width-image img {
            max-width: 100%;
            height: auto;
            border-radius: 5px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.15);
        }
        .code-block {
            background: #f4f4f4;
            border-left: 4px solid #003262;
            padding: 15px;
            margin: 15px 0;
            overflow-x: auto;
            font-family: 'Courier New', monospace;
            font-size: 0.9em;
        }
        .note {
            background: #fff9e6;
            border-left: 4px solid #FDB515;
            padding: 15px;
            margin: 15px 0;
        }
        .observation {
            background: #e6f3ff;
            border-left: 4px solid #003262;
            padding: 15px;
            margin: 15px 0;
        }
        ol, ul {
            margin: 10px 0;
            padding-left: 25px;
        }
        li {
            margin: 5px 0;
        }
    </style>
</head>
<body>
    <h1>CS 180 Project 2: Fun with Filters and Frequencies</h1>
    
    <div class="section">
        <h2>Overview</h2>
        <p>This project implements fundamental image processing techniques using convolution and frequency domain operations. In Part 1, I focus on edge detection through finite difference operators and Derivative of Gaussian (DoG) filters. In Part 2, I explore frequency-based image manipulation including image sharpening, hybrid images combining different frequency bands, and multiresolution blending using Gaussian and Laplacian stacks.</p>
    </div>

    <div class="section">
        <h2>Part 1: Fun with Filters</h2>
        
        <h3>1.1 Finite Difference Operator</h3>
        <p>I implemented convolution from scratch using both 4-loop and 2-loop approaches, then compared them with scipy.signal.convolve2d. I used proper padding with zero fill values. After that, I took a picture of a cup, read it as grayscale, created a 9x9 box filter, and convolved the picture with the box filter. I also applied the finite difference operators D<sub>x</sub> and D<sub>y</sub>.</p>
        
        <div class="code-block">
<strong>4-Loop Convolution Implementation:</strong>
<pre><code>def convolution_4loops(img, kernel):
    if len(img.shape) == 3:
        img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
    
    img = img.astype(np.float64)
    kernel = kernel.astype(np.float64)
    kernel = np.flip(kernel)  # Important: flip kernel for convolution
    
    img_h, img_w = img.shape
    ker_h, ker_w = kernel.shape
    
    # Calculate padding
    pad_h = ker_h // 2
    pad_w = ker_w // 2
    
    # Pad the image with zeros
    padded_img = np.pad(img, ((pad_h, pad_h), (pad_w, pad_w)), mode='constant')
    
    # Initialize output
    output = np.zeros((img_h, img_w))
    
    # 4 nested loops
    for i in range(img_h):
        for j in range(img_w):
            for ki in range(ker_h):
                for kj in range(ker_w):
                    output[i, j] += padded_img[i + ki, j + kj] * kernel[ki, kj]
    
    return output
</code></pre>

<strong>2-Loop Convolution Implementation:</strong>
<pre><code>def convolution_2loops(img, kernel):
    if len(img.shape)==3:
        img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
    img = img.astype(np.float64)
    kernel = kernel.astype(np.float64)
    kernel = np.flip(kernel)
    
    img_h, img_w = img.shape
    ker_h, ker_w = kernel.shape
    pad_h = ker_h // 2
    pad_w = ker_w // 2
    
    padded_img = np.pad(img, ((pad_h, pad_h), (pad_w, pad_w)), mode='constant')
    output = np.zeros((img_h, img_w))
    
    # 2 nested loops with vectorized computation
    for i in range(img_h):
        for j in range(img_w):
            region = padded_img[i:i+ker_h, j:j+ker_w]
            output[i, j] = np.sum(region * kernel)
    
    return output
</code></pre>
        </div>
        
        <div class="full-width-image">
            <img src="code/results/part1/box_filter_comparison.png" alt="Box Filter Comparison">
            <div class="image-caption">9x9 Box Filter: Comparison of all three implementations (4-loops, 2-loops, and SciPy)</div>
        </div>
        
        <div class="observation">
            <strong>Verification:</strong> All three implementations produce nearly identical results with mean absolute differences less than 0.000001, confirming correctness.
        </div>

        <div class="full-width-image">
            <img src="code/results/part1/derivative_filters_comparison.png" alt="Derivative Filters Comparison">
            <div class="image-caption">Finite difference operators Dx and Dy: Comparison of all three implementations</div>
        </div>

        <h3>1.2 Derivative of Gaussian (DoG) Filter</h3>
        <p>Applied finite difference operators Dx=[1,0,-1] and Dy=[[1],[0],[-1]] to detect edges in the cameraman image. I computed partial derivatives in x and y directions, then calculated the gradient magnitude and binarized it using a threshold at the 95th percentile to suppress noise while preserving real edges.</p>
        
        <div class="full-width-image">
            <img src="code/results/part1/finite_difference_edges.png" alt="Edge Detection">
            <div class="image-caption">Edge detection using finite difference operators: partial derivatives and gradient magnitude</div>
        </div>
        
        <div class="observation">
            <strong>Observations:</strong>
            <ul>
                <li>Partial derivative ∂I/∂x captures vertical edges (transitions in horizontal direction)</li>
                <li>Partial derivative ∂I/∂y captures horizontal edges (transitions in vertical direction)</li>
                <li>Gradient magnitude combines both to show all edge directions</li>
                <li>Threshold at 95th percentile produces clean edges while suppressing noise</li>
            </ul>
        </div>

        <h3>1.3 Derivative of Gaussian (DoG) Filter</h3>
        <p>To reduce noise in edge detection, I applied Gaussian smoothing before computing derivatives. I constructed Gaussian filters using cv2.getGaussianKernel with kernel_size=15 and sigma=2.0, built DoG filters by convolving the Gaussian with difference operators, and visualized them.</p>
        
        <p>I compared two approaches: (1) blur first, then apply difference operators, and (2) create DoG filters and apply them directly. Both methods produce identical results, demonstrating that convolution is associative.</p>
        
        <div class="full-width-image">
            <img src="code/results/part1/dog_edge_detection_comparison.png" alt="DoG Edge Detection">
            <div class="image-caption">Comparison of edge detection methods: without Gaussian (Part 1.2), with Gaussian smoothing, and DoG filters. Bottom row shows the DoG_x, DoG_y filters and the original Gaussian filter.</div>
        </div>
        
        <div class="note">
            <strong>Mathematical Equivalence:</strong> I verified that convolving the image with a Derivative of Gaussian (DoG) filter produces identical results to first blurring then computing derivatives. Maximum difference between the two approaches: less than 0.001.
        </div>
        
        <div class="observation">
            <strong>Key Differences from Part 1.2:</strong>
            <ul>
                <li><strong>Noise Reduction:</strong> Gaussian preprocessing significantly reduces background noise</li>
                <li><strong>Cleaner Edges:</strong> Main object boundaries are more continuous and well-defined</li>
                <li><strong>Better Connectivity:</strong> Edge pixels form more coherent structures</li>
                <li><strong>Edge Density:</strong> Original method yields approximately 5% edge pixels vs 3.5% with Gaussian smoothing</li>
            </ul>
        </div>
    </div>

    <div class="section">
        <h2>Part 2: Fun with Frequencies</h2>
        
        <h3>2.1 Image Sharpening</h3>
        <p>Implemented unsharp masking: sharpen = original + α × (original - blurred). The technique enhances high-frequency details by adding back the high-pass filtered content. I tested different parameter combinations and selected sigma=2.0 and alpha=1.5 for optimal balance between sharpening effect and artifact suppression.</p>
        
        <div class="full-width-image">
            <img src="code/results/part2/part1_taj_mahal_sharpening.png" alt="Taj Mahal Sharpening">
            <div class="image-caption">Taj Mahal: Original → Blurred → High Frequencies → Sharpened</div>
        </div>
        
        <div class="full-width-image">
            <img src="code/results/part2/part2_orange_cat_sharpening.png" alt="Orange Cat Sharpening">
            <div class="image-caption">Orange Cat: Demonstrating unsharp masking on my chosen image</div>
        </div>
        
        <h4>Evaluation: Can We Recover Lost Details?</h4>
        <div class="full-width-image">
            <img src="code/results/part2/part3_evaluation_test.png" alt="Sharpening Evaluation">
            <div class="image-caption">Testing: Sharp → Blur → Re-sharpen on white cat image</div>
        </div>
        
        <div class="observation">
            <strong>Critical Insight:</strong> Once information is lost through blurring, it cannot be perfectly recovered. Re-sharpening a blurred image often introduces artifacts and cannot restore the original detail. Unsharp masking works best for enhancing existing sharp images, not for recovering lost information.
        </div>

        <h3>2.2 Hybrid Images</h3>
        <p>Hybrid images are static images that change in interpretation as a function of viewing distance. The basic idea is to blend the high-frequency portion of one image with the low-frequency portion of another. I used sigma1=12 for low-pass filtering and sigma2=6 for high-pass filtering.</p>
        
        <h4>Derek + Nutmeg (with Frequency Analysis)</h4>
        <p>For my favorite result, I show the complete frequency analysis as required.</p>
        
        <div class="full-width-image">
            <img src="code/results/part2/hybrid_fft_analysis.jpg" alt="FFT Analysis">
            <div class="image-caption">Frequency analysis showing original FFTs, filtered images (low-pass Derek, high-pass Nutmeg), and hybrid result FFT</div>
        </div>
        
        <div class="observation">
            <strong>Frequency Domain Analysis:</strong>
            <ul>
                <li><strong>Low-pass filtered (Derek):</strong> Energy concentrated at center, high frequencies attenuated</li>
                <li><strong>High-pass filtered (Nutmeg):</strong> Energy in outer regions, DC component suppressed</li>
                <li><strong>Hybrid FFT:</strong> Combines low-frequency center from Derek with high-frequency periphery from Nutmeg</li>
            </ul>
        </div>
        
        <div class="full-width-image">
            <img src="code/results/part2/hybrid_extra_result.jpg" alt="Derek-Nutmeg Hybrid">
            <div class="image-caption">Derek + Nutmeg hybrid result: At close distance you see Nutmeg (high frequencies), at far distance you see Derek (low frequencies)</div>
        </div>

        <h4>Additional Hybrid Images</h4>
        
        <div class="full-width-image">
            <img src="code/results/part2/hybrid_tiger_cat.jpg" alt="Tiger-Cat Hybrid">
            <div class="image-caption">Tiger (low frequencies) + Cat (high frequencies)</div>
        </div>

        <h3>2.3 Gaussian and Laplacian Stacks</h3>
        <p>Implemented Gaussian and Laplacian stacks with N=5 levels and sigma=8. The key difference from pyramids is that stacks never downsample, so all levels remain the same dimensions. I applied these stacks to recreate Figure 3.42 from Szelski's textbook for the apple and orange blend.</p>
        
        <div class="note">
            <strong>Implementation Details:</strong> For Gaussian stack, each level applies progressively stronger blur. For Laplacian stack, each level is the difference between consecutive Gaussian levels, capturing different frequency bands. The final Laplacian level contains the lowest frequencies (same as final Gaussian level).
            
            <p><strong>Parameter Selection (sigma=8):</strong> I selected sigma=8 to achieve proper frequency band separation. With this parameter, the Laplacian layers show the expected sparse representation with high-frequency layers (L0-L3) having standard deviations of 0.006-0.04, while the Gaussian residual (L4) contains most energy with std ≈ 0.19-0.22.</p>
        </div>
        
        <div class="full-width-image">
            <img src="code/results/part2/gaussian_stacks.png" alt="Gaussian Stacks">
            <div class="image-caption">Gaussian stacks for Apple and Orange (N=5 levels, sigma=8)</div>
        </div>
        
        <div class="full-width-image">
            <img src="code/results/part2/laplacian_stacks.png" alt="Laplacian Stacks">
            <div class="image-caption">Laplacian stacks for Apple and Orange. Note: Laplacian layers (L0-L3) shown with +0.5 offset for visualization; final layer (L4) is the Gaussian residual</div>
        </div>

        <h3>2.4 Multiresolution Blending</h3>
        <p>Using the Laplacian stacks from Part 2.3, I blended images seamlessly. For each level of the Laplacian stack, I applied the Gaussian-blurred mask to blend the two images, then summed all levels to create the final result.</p>
        
        <h4>The Oraple Result</h4>
        <div class="full-width-image">
            <img src="code/results/part2/oraple_result.png" alt="Oraple">
            <div class="image-caption">The Oraple: Apple + Orange blended with vertical seam</div>
        </div>
        
        <h4>Laplacian Blending Process (Recreating Figure 3.42)</h4>
        <p>As required by the assignment, I illustrate the blending process showing masked Laplacian levels. This demonstrates how each frequency band is blended separately using the Gaussian-blurred mask at that level.</p>
        
        <div class="full-width-image">
            <img src="code/results/part2/laplacian_blending_oraple.png" alt="Oraple Blending Process">
            <div class="image-caption">Laplacian blending process: Each row shows Apple L[i]×Mask, Orange L[i]×(1-Mask), and Blended L[i] for levels 0-4. Note: Layers 0-3 shown with +0.5 offset for visualization; layer 4 is Gaussian residual.</div>
        </div>
        

        
        <h4>Additional Blends with Irregular Masks</h4>
        
        <div class="full-width-image">
            <img src="code/results/part2/water_mountain_result.png" alt="Water Mountain Blend">
            <div class="image-caption">Water + Mountain with horizontal mask (N=5, sigma=8)</div>
        </div>
        
        <div class="full-width-image">
            <img src="code/results/part2/result_cloud_women.png" alt="Cloud Woman Blend">
            <div class="image-caption">Cloud sky replacement with irregular mask based on brightness detection</div>
        </div>
        
        <div class="full-width-image">
            <img src="code/results/part2/koala_in_phone_detailed.png" alt="Koala in Phone">
            <div class="image-caption">Koala composited into phone screen with rectangular mask. Shows original images, resized koala, mask, and final blend.</div>
        </div>
        
        <div class="full-width-image">
            <img src="code/results/part2/laplacian_blending_koala_phone.png" alt="Koala Phone Blending Process">
            <div class="image-caption">Laplacian blending process for Koala + Phone: Each row shows masked Laplacian levels and blended results (N=5, sigma=2)</div>
        </div>
    </div>


</body>
</html>