<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>CS180 Project 5: Diffusion Models</title>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <style>
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body {
            font-family: 'Georgia', serif;
            line-height: 1.7;
            color: #333;
            max-width: 1000px;
            margin: 0 auto;
            padding: 20px;
            background: #fafafa;
        }
        header {
            text-align: center;
            padding: 40px 0;
            border-bottom: 2px solid #003262;
            margin-bottom: 40px;
        }
        h1 { font-size: 2.2em; color: #003262; margin-bottom: 10px; }
        .subtitle { font-size: 1.1em; color: #666; }
        .author { margin-top: 15px; font-style: italic; color: #555; }
        h2 {
            font-size: 1.6em;
            color: #003262;
            margin: 40px 0 20px;
            padding-bottom: 10px;
            border-bottom: 1px solid #ddd;
        }
        h3 { font-size: 1.3em; color: #444; margin: 30px 0 15px; }
        h4 { font-size: 1.1em; color: #555; margin: 20px 0 10px; }
        p { margin-bottom: 15px; text-align: justify; }
        .image-container { text-align: center; margin: 25px 0; }
        .image-container img {
            max-width: 100%;
            border-radius: 4px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }
        .image-row {
            display: flex;
            justify-content: center;
            gap: 10px;
            flex-wrap: wrap;
            margin: 25px 0;
        }
        .image-row img {
            height: 128px;
            border-radius: 4px;
            box-shadow: 0 1px 4px rgba(0,0,0,0.1);
        }
        .image-row-large img { height: 180px; }
        .caption { font-size: 0.9em; color: #666; margin-top: 8px; font-style: italic; text-align: center; }
        .inline-code {
            background: #eee;
            padding: 2px 6px;
            border-radius: 3px;
            font-family: 'Consolas', monospace;
            font-size: 0.9em;
        }
        .formula-box {
            background: #f5f5f5;
            border-left: 4px solid #003262;
            padding: 15px 20px;
            margin: 20px 0;
            border-radius: 0 6px 6px 0;
            text-align: center;
        }
        .section { margin-bottom: 50px; }
        hr { border: none; border-top: 1px solid #ddd; margin: 40px 0; }
        footer {
            text-align: center;
            padding: 40px 0;
            border-top: 1px solid #ddd;
            margin-top: 50px;
            color: #888;
        }
    </style>
</head>
<body>
    <header>
        <h1>Project 5: Diffusion Models</h1>
        <p class="subtitle">CS180: Intro to Computer Vision and Computational Photography</p>
        <p class="author">Chunrui Huang &middot; Fall 2025</p>
    </header>

    <!-- Part A -->
    <section class="section">
        <h2>Part A: The Power of Diffusion Models</h2>
        <p>
            In Part A, I explored text-to-image generation using the pretrained DeepFloyd IF diffusion model, 
            implementing sampling loops, denoising algorithms, and creative applications like visual anagrams and hybrid images.
        </p>
    </section>

    <hr>

    <!-- Part 0: Setup -->
    <section id="part0" class="section">
        <h2>Part 0: Setup</h2>
        
        <p>
            I used the <strong>DeepFloyd IF</strong> diffusion model, a two-stage model where stage 1 produces 64x64 images 
            and stage 2 upsamples them to 256x256. For reproducibility, I set my random seed to <strong>8</strong>.
        </p>

        <h3>Sampling from the Model</h3>
        <p>
            I tested the model with three prompts, varying the number of inference steps (10, 20, 50) to observe 
            the quality-speed tradeoff.
        </p>

        <h4>"a tired cat in class"</h4>
        <div class="image-row">
            <img src="code/a_tired_cat_in_class_steps10.png" alt="steps=10">
            <img src="code/a_tired_cat_in_class_steps20.png" alt="steps=20">
            <img src="code/a_tired_cat_in_class_steps50.png" alt="steps=50">
        </div>
        <p class="caption">num_inference_steps = 10  ->  20  ->  50</p>

        <h4>"a fashionable dog eating dinner"</h4>
        <div class="image-row">
            <img src="code/a_fashionable_dog_eating_dinner_steps10.png" alt="steps=10">
            <img src="code/a_fashionable_dog_eating_dinner_steps20.png" alt="steps=20">
            <img src="code/a_fashionable_dog_eating_dinner_steps50.png" alt="steps=50">
        </div>
        <p class="caption">num_inference_steps = 10  ->  20  ->  50</p>

        <h4>"a cozy bookstore"</h4>
        <div class="image-row">
            <img src="code/a_cozy_bookstore_steps10.png" alt="steps=10">
            <img src="code/a_cozy_bookstore_steps20.png" alt="steps=20">
            <img src="code/a_cozy_bookstore_steps50.png" alt="steps=50">
        </div>
        <p class="caption">num_inference_steps = 10  ->  20  ->  50</p>

        <p>
            <strong>Observations:</strong> With fewer steps (10), images appear blurry. At 50 steps, the model produces 
            sharp, coherent images that closely match the text prompts.
        </p>
    </section>

    <hr>

    <!-- Part 1.1: Forward Process -->
    <section id="part1-1" class="section">
        <h2>Part 1.1: Implementing the Forward Process</h2>
        
        <p>
            The forward diffusion process adds Gaussian noise to a clean image using the closed-form formula:
        </p>

        <div class="formula-box">
            \[ x_t = \sqrt{\bar{\alpha}_t} \cdot x_0 + \sqrt{1 - \bar{\alpha}_t} \cdot \epsilon, \quad \text{where } \epsilon \sim \mathcal{N}(0, 1) \]
        </div>

        <p>
            I implemented <span class="inline-code">forward(im, t)</span> which takes an image and timestep, 
            then returns the noisy image by scaling the original with \(\sqrt{\bar{\alpha}_t}\) and adding scaled Gaussian noise.
        </p>
        <h3>Results: Noisy Images at t = [250, 500, 750]</h3>
        <div class="image-row">
            <img src="code/part1_1_noisy_t250.png" alt="t=250">
            <img src="code/part1_1_noisy_t500.png" alt="t=500">
            <img src="code/part1_1_noisy_t750.png" alt="t=750">
        </div>
        <p class="caption">t=250  ->  t=500  ->  t=750</p>
    </section>

    <hr>

    <!-- Part 1.2: Classical Denoising -->
    <section id="part1-2" class="section">
        <h2>Part 1.2: Classical Denoising</h2>
        
        <p>
            I applied Gaussian blur (kernel_size=5, sigma=2) as a classical denoising method. 
            This low-pass filter smooths noise but cannot distinguish between signal and noise, 
            resulting in blurry outputs that fail to recover image structure.
        </p>

        <div class="image-row">
<div class="image-row">
    <div class="image-item">
        <img src="code/part1_2_Noisy_t250.png" alt="Noisy t=250">
        <p class="caption">Noisy Campanile at t=250</p>
    </div>
    <div class="image-item">
        <img src="code/part1_2_Noisy_t500.png" alt="Noisy t=500">
        <p class="caption">Noisy Campanile at t=500</p>
    </div>
    <div class="image-item">
        <img src="code/part1_2_Noisy_t750.png" alt="Noisy t=750">
        <p class="caption">Noisy Campanile at t=750</p>
    </div>
</div>
<div class="image-row">
    <div class="image-item">
        <img src="code/part1_2_Blur_t250.png" alt="Blur t=250">
        <p class="caption">Gaussian Blur Denoising<br>at t=250</p>
    </div>
    <div class="image-item">
        <img src="code/part1_2_Blur_t500.png" alt="Blur t=500">
        <p class="caption">Gaussian Blur Denoising<br>at t=500</p>
    </div>
    <div class="image-item">
        <img src="code/part1_2_Blur_t750.png" alt="Blur t=750">
        <p class="caption">Gaussian Blur Denoising<br>at t=750</p>
    </div>
</div>
    <hr>

    <!-- Part 1.3: One-Step Denoising -->
    <section id="part1-3" class="section">
        <h2>Part 1.3: One-Step Denoising</h2>
        
        <p>
            Using the pretrained UNet, I estimated the noise in the image and recovered the clean image in one step:
        </p>

        <div class="formula-box">
            \[ \hat{x}_0 = \frac{x_t - \sqrt{1 - \bar{\alpha}_t} \cdot \hat{\epsilon}}{\sqrt{\bar{\alpha}_t}} \]
        </div>

        <p>
            The UNet takes the noisy image, timestep, and text prompt embeddings as input to predict the noise.
        </p>

<div class="image-row">
    <div class="image-item">
        <img src="code/part1_3_Original.png" alt="Original">
        <p class="caption">Original</p>
    </div>
    <div class="image-item">
        <img src="code/part1_3_Noisy_t250.png" alt="Noisy t=250">
        <p class="caption">Noisy<br>at t=250</p>
    </div>
    <div class="image-item">
        <img src="code/part1_3_Denoised_t250.png" alt="Denoised t=250">
        <p class="caption">One-Step Denoised<br>at t=250</p>
    </div>
    <div class="image-item">
        <img src="code/part1_3_Noisy_t500.png" alt="Noisy t=500">
        <p class="caption">Noisy<br>at t=500</p>
    </div>
    <div class="image-item">
        <img src="code/part1_3_Denoised_t500.png" alt="Denoised t=500">
        <p class="caption">One-Step Denoised<br>at t=500</p>
    </div>
    <div class="image-item">
        <img src="code/part1_3_Noisy_t750.png" alt="Noisy t=750">
        <p class="caption">Noisy<br>at t=750</p>
    </div>
    <div class="image-item">
        <img src="code/part1_3_Denoised_t750.png" alt="Denoised t=750">
        <p class="caption">One-Step Denoised<br>at t=750</p>
    </div>
</div>
    </section>

    <hr>

    <!-- Part 1.4: Iterative Denoising -->
    <section id="part1-4" class="section">
        <h2>Part 1.4: Iterative Denoising</h2>
        
        <p>
            Instead of one-step denoising, I implemented iterative denoising using the DDPM update rule. 
            Starting from a noisy image, I created strided timesteps (stride=30 from 990 to 0) and iteratively 
            refined the image using the formula:
        </p>

        <div class="formula-box">
            \[ x_{t'} = \frac{\sqrt{\bar{\alpha}_{t'}} \cdot \beta_t}{1 - \bar{\alpha}_t} \cdot \hat{x}_0 + \frac{\sqrt{\alpha_t} \cdot (1 - \bar{\alpha}_{t'})}{1 - \bar{\alpha}_t} \cdot x_t + v_\sigma \]
        </div>

        <p>
            At each step, I computed \(\alpha\), \(\beta\) from the noise schedule, estimated \(\hat{x}_0\) from the current noisy image, 
            then interpolated between \(\hat{x}_0\) and \(x_t\) with added variance.
        </p>
        <h3>Denoising Process</h3>
        <p>The images below show the noisy Campanile at different timesteps (gradually becoming less noisy):</p>
        <div class="image-row">
            <img src="code/part1_4_noisy_t690.png" alt="t=690">
            <img src="code/part1_4_noisy_t540.png" alt="t=540">
            <img src="code/part1_4_noisy_t390.png" alt="t=390">
            <img src="code/part1_4_noisy_t240.png" alt="t=240">
            <img src="code/part1_4_noisy_t90.png" alt="t=90">
        </div>
        <p class="caption">Noisy Campanile at t=690  ->  t=540  ->  t=390  ->  t=240  ->  t=90</p>

        <h3>Comparison</h3>
        <div class="image-row">
            <img src="code/part1_4_original.png" alt="Original">
            <img src="code/part1_4_iterative.png" alt="Iteratively Denoised">
            <img src="code/part1_4_one_step.png" alt="One-Step Denoised">
            <img src="code/part1_4_gaussian_blur.png" alt="Gaussian Blurred">
        </div>
        <p class="caption">Original  ->  Iteratively Denoised  ->  One-Step Denoised  ->  Gaussian Blurred</p>
        
        <p>
            <strong>Observations:</strong> Iterative denoising produces the cleanest result. One-step denoising 
            shows more artifacts, and Gaussian blur fails to recover structure.
        </p>
    </section>

    <hr>

    <!-- Part 1.5: Diffusion Model Sampling -->
    <section id="part1-5" class="section">
        <h2>Part 1.5: Diffusion Model Sampling</h2>
        
        <p>
            By setting i_start=0 and starting from pure Gaussian noise, I used <span class="inline-code">iterative_denoise</span> 
            to generate entirely new images from scratch with the prompt "a high quality photo".
        </p>

        <div class="image-row">
            <img src="code/part1_5_Sample_1.png" alt="Sample 1">
            <img src="code/part1_5_Sample_2.png" alt="Sample 2">
            <img src="code/part1_5_Sample_3.png" alt="Sample 3">
            <img src="code/part1_5_Sample_4.png" alt="Sample 4">
            <img src="code/part1_5_Sample_5.png" alt="Sample 5">
        </div>
        <p class="caption">5 samples generated from pure noise</p>
    </section>

    <hr>

    <!-- Part 1.6: Classifier-Free Guidance -->
    <section id="part1-6" class="section">
        <h2>Part 1.6: Classifier-Free Guidance (CFG)</h2>
        
        <p>
            To improve sample quality, I implemented CFG which combines conditional and unconditional noise estimates:
        </p>

        <div class="formula-box">
            \[ \epsilon = \epsilon_u + \gamma \cdot (\epsilon_c - \epsilon_u), \quad \text{with } \gamma = 7 \]
        </div>

        <p>
            At each denoising step, I computed both conditional (with prompt) and unconditional (empty string) 
            noise estimates, then extrapolated beyond the conditional estimate for stronger guidance.
        </p>

        <div class="image-row">
            <img src="code/part1_6_Sample_1.png" alt="Sample 1">
            <img src="code/part1_6_Sample_2.png" alt="Sample 2">
            <img src="code/part1_6_Sample_3.png" alt="Sample 3">
            <img src="code/part1_6_Sample_4.png" alt="Sample 4">
            <img src="code/part1_6_Sample_5.png" alt="Sample 5">
        </div>
        <p class="caption">5 CFG-guided samples (gamma=7)</p>
    </section>

    <hr>

    <!-- Part 1.7: Image-to-Image Translation -->
    <section id="part1-7" class="section">
        <h2>Part 1.7: Image-to-Image Translation</h2>
        
        <p>
            Using SDEdit, I edited existing images by adding noise to a specific level (controlled by i_start) 
            and then denoising with CFG. Lower i_start means more noise and more creative freedom; 
            higher i_start preserves more of the original structure.
        </p>

        <h3>Campanile</h3>
        <div class="image-row">
            <img src="code/part1_7_campanile_Original.png" alt="Original">
            <img src="code/part1_7_campanile_i1.png" alt="i=1">
            <img src="code/part1_7_campanile_i3.png" alt="i=3">
            <img src="code/part1_7_campanile_i5.png" alt="i=5">
            <img src="code/part1_7_campanile_i7.png" alt="i=7">
            <img src="code/part1_7_campanile_i10.png" alt="i=10">
            <img src="code/part1_7_campanile_i20.png" alt="i=20">
        </div>
        <p class="caption">Original  ->  i_start = 1, 3, 5, 7, 10, 20</p>

        <h3>My Test Image 1</h3>
        <div class="image-row">
            <img src="code/part1_7_myimg1_Original.png" alt="Original">
            <img src="code/part1_7_myimg1_i1.png" alt="i=1">
            <img src="code/part1_7_myimg1_i3.png" alt="i=3">
            <img src="code/part1_7_myimg1_i5.png" alt="i=5">
            <img src="code/part1_7_myimg1_i7.png" alt="i=7">
            <img src="code/part1_7_myimg1_i10.png" alt="i=10">
            <img src="code/part1_7_myimg1_i20.png" alt="i=20">
        </div>
        <p class="caption">Original  ->  i_start = 1, 3, 5, 7, 10, 20</p>

        <h3>My Test Image 2</h3>
        <div class="image-row">
            <img src="code/part1_7_myimg2_Original.png" alt="Original">
            <img src="code/part1_7_myimg2_i1.png" alt="i=1">
            <img src="code/part1_7_myimg2_i3.png" alt="i=3">
            <img src="code/part1_7_myimg2_i5.png" alt="i=5">
            <img src="code/part1_7_myimg2_i7.png" alt="i=7">
            <img src="code/part1_7_myimg2_i10.png" alt="i=10">
            <img src="code/part1_7_myimg2_i20.png" alt="i=20">
        </div>
        <p class="caption">Original  ->  i_start = 1, 3, 5, 7, 10, 20</p>
    </section>

    <hr>

    <!-- Part 1.7.1: Hand-Drawn and Web Images -->
    <section id="part1-7-1" class="section">
        <h2>Part 1.7.1: Editing Hand-Drawn and Web Images</h2>

        <p>
            SDEdit works especially well for projecting non-photorealistic inputs onto the natural image manifold.
        </p>

        <h3>Web Image</h3>
        <div class="image-row">
            <img src="code/part1_7_1_web_1.png" alt="i=1">
            <img src="code/part1_7_1_web_3.png" alt="i=3">
            <img src="code/part1_7_1_web_5.png" alt="i=5">
            <img src="code/part1_7_1_web_7.png" alt="i=7">
            <img src="code/part1_7_1_web_10.png" alt="i=10">
            <img src="code/part1_7_1_web_20.png" alt="i=20">
            <img src="code/part1_7_1_web_25.png" alt="i=25">
        </div>
        <p class="caption">i_start = 1, 3, 5, 7, 10, 20, 25</p>

        <h3>Hand-Drawn Image 1</h3>
        <div class="image-row">
            <img src="code/part1_7_1_drawn1_1.png" alt="i=1">
            <img src="code/part1_7_1_drawn1_3.png" alt="i=3">
            <img src="code/part1_7_1_drawn1_5.png" alt="i=5">
            <img src="code/part1_7_1_drawn1_7.png" alt="i=7">
            <img src="code/part1_7_1_drawn1_10.png" alt="i=10">
            <img src="code/part1_7_1_drawn1_20.png" alt="i=20">
            <img src="code/part1_7_1_drawn1_23.png" alt="i=23">
        </div>
        <p class="caption">i_start = 1, 3, 5, 7, 10, 20, 23</p>

        <h3>Hand-Drawn Image 2</h3>
        <div class="image-row">
            <img src="code/part1_7_1_drawn2_1.png" alt="i=1">
            <img src="code/part1_7_1_drawn2_3.png" alt="i=3">
            <img src="code/part1_7_1_drawn2_5.png" alt="i=5">
            <img src="code/part1_7_1_drawn2_7.png" alt="i=7">
            <img src="code/part1_7_1_drawn2_10.png" alt="i=10">
            <img src="code/part1_7_1_drawn2_20.png" alt="i=20">
            <img src="code/part1_7_1_drawn2_30.png" alt="i=30">
        </div>
        <p class="caption">i_start = 1, 3, 5, 7, 10, 20, 30</p>
    </section>

    <hr>

    <!-- Part 1.7.2: Inpainting -->
    <section id="part1-7-2" class="section">
        <h2>Part 1.7.2: Inpainting</h2>
        
        <p>
            I implemented inpainting using the RePaint approach: at each denoising step, I kept pixels outside 
            the mask from the original image (noised to the current timestep) while allowing the model to 
            generate new content inside the masked region.
        </p>

        <h3>Campanile Inpainting</h3>
        <div class="image-row">
            <img src="code/part1_7_2_campanile_original.png" alt="Original">
            <img src="code/part1_7_2_campanile_mask.png" alt="Mask">
            <img src="code/part1_7_2_campanile_inpainted.png" alt="Inpainted">
        </div>
        <p class="caption">Original  ->  Mask  ->  Inpainted</p>

        <h3>My Image 1 Inpainting</h3>
        <div class="image-row">
            <img src="code/part1_7_2_myimg3_original.png" alt="Original">
            <img src="code/part1_7_2_myimg3_mask.png" alt="Mask">
            <img src="code/part1_7_2_myimg3_inpainted.png" alt="Inpainted">
        </div>
        <p class="caption">Original  ->  Mask  ->  Inpainted</p>

        <h3>My Image 2 Inpainting</h3>
        <div class="image-row">
            <img src="code/part1_7_2_myimg4_original.png" alt="Original">
            <img src="code/part1_7_2_myimg4_mask.png" alt="Mask">
            <img src="code/part1_7_2_myimg4_inpainted.png" alt="Inpainted">
        </div>
        <p class="caption">Original  ->  Mask  ->  Inpainted</p>
    </section>

    <hr>

    <!-- Part 1.7.3: Text-Conditional Image-to-Image -->
    <section id="part1-7-3" class="section">
        <h2>Part 1.7.3: Text-Conditional Image-to-Image Translation</h2>

        <p>
            Instead of a generic prompt, I used specific text prompts to guide the transformation 
            toward new semantic content while preserving spatial structure.
        </p>

        <h3>Campanile  ->  "an oil painting of people around a campfire"</h3>
        <div class="image-row">
            <img src="code/part1_7_3_campanile_Original.png" alt="Original">
            <img src="code/part1_7_3_campanile_i1.png" alt="i=1">
            <img src="code/part1_7_3_campanile_i3.png" alt="i=3">
            <img src="code/part1_7_3_campanile_i5.png" alt="i=5">
            <img src="code/part1_7_3_campanile_i7.png" alt="i=7">
            <img src="code/part1_7_3_campanile_i10.png" alt="i=10">
            <img src="code/part1_7_3_campanile_i20.png" alt="i=20">
        </div>
        <p class="caption">Original  ->  i_start = 1, 3, 5, 7, 10, 20</p>

        <h3>My Image 1  ->  "a pencil sketch of a young woman"</h3>
        <div class="image-row">
            <img src="code/part1_7_3_myimg1_Original.png" alt="Original">
            <img src="code/part1_7_3_myimg1_i1.png" alt="i=1">
            <img src="code/part1_7_3_myimg1_i3.png" alt="i=3">
            <img src="code/part1_7_3_myimg1_i5.png" alt="i=5">
            <img src="code/part1_7_3_myimg1_i7.png" alt="i=7">
            <img src="code/part1_7_3_myimg1_i10.png" alt="i=10">
            <img src="code/part1_7_3_myimg1_i20.png" alt="i=20">
        </div>
        <p class="caption">Original  ->  i_start = 1, 3, 5, 7, 10, 20</p>

        <h3>My Image 2  ->  "a watercolor of mountains"</h3>
        <div class="image-row">
            <img src="code/part1_7_3_myimg2_Original.png" alt="Original">
            <img src="code/part1_7_3_myimg2_i1.png" alt="i=1">
            <img src="code/part1_7_3_myimg2_i3.png" alt="i=3">
            <img src="code/part1_7_3_myimg2_i5.png" alt="i=5">
            <img src="code/part1_7_3_myimg2_i7.png" alt="i=7">
            <img src="code/part1_7_3_myimg2_i10.png" alt="i=10">
            <img src="code/part1_7_3_myimg2_i20.png" alt="i=20">
        </div>
        <p class="caption">Original  ->  i_start = 1, 3, 5, 7, 10, 20</p>
    </section>

    <hr>

    <!-- Part 1.8: Visual Anagrams -->
    <section id="part1-8" class="section">
        <h2>Part 1.8: Visual Anagrams</h2>
        
        <p>
            Visual anagrams are images that show one thing upright and a different thing when flipped 180&deg;. 
            I implemented this by computing noise estimates for both orientations and averaging them:
        </p>

        <div class="formula-box">
            \[ \epsilon = \frac{\epsilon_1 + \text{flip}(\epsilon_2)}{2} \]
        </div>

        <p>
            At each step, I computed the CFG noise estimate for prompt 1 on the normal image, 
            then flipped the image, computed the noise for prompt 2, flipped the result back, 
            and averaged the two estimates.
        </p>

        <h3>Illusion 1: Young Woman  <->  Flowers in a Vase</h3>
        <div class="image-row image-row-large">
            <img src="code/part1_8_illusion1_normal.png" alt="Normal">
            <img src="code/part1_8_illusion1_flipped.png" alt="Flipped">
        </div>
        <p class="caption">"a pencil sketch of a young woman"  <->  "a pencil sketch of flowers in a vase"</p>

        <h3>Illusion 2: Mountains  <->  Valleys</h3>
        <div class="image-row image-row-large">
            <img src="code/part1_8_illusion2_normal.png" alt="Normal">
            <img src="code/part1_8_illusion2_flipped.png" alt="Flipped">
        </div>
        <p class="caption">"a watercolor of mountains"  <->  "a watercolor of valleys"</p>
    </section>

    <hr>

    <!-- Part 1.9: Hybrid Images -->
    <section id="part1-9" class="section">
        <h2>Part 1.9: Hybrid Images</h2>
        
        <p>
            Hybrid images appear as one thing from afar (low frequencies) and another up close (high frequencies). 
            I combined noise estimates using frequency filtering:
        </p>

        <div class="formula-box">
            \[ \epsilon = f_{\text{lowpass}}(\epsilon_1) + f_{\text{highpass}}(\epsilon_2) \]
        </div>

        <p>
            I applied Gaussian blur (kernel_size=33, sigma=2) to get the low-pass component from prompt 1, 
            and subtracted the blurred version from prompt 2's estimate to get the high-pass component.
        </p>

        <h3>Hybrid 1: Skull (far) + Waterfalls (close)</h3>
        <div class="image-container">
            <img src="code/part1_9_hybrid1.png" alt="Hybrid 1" style="height: 256px;">
        </div>
        <p class="caption">"a lithograph of a skull" (low freq) + "a lithograph of waterfalls" (high freq)</p>

        <h3>Hybrid 2: Old Man (far) + Campfire (close)</h3>
        <div class="image-container">
            <img src="code/part1_9_hybrid2.png" alt="Hybrid 2" style="height: 256px;">
        </div>
        <p class="caption">"an oil painting of an old man" (low freq) + "an oil painting of people around a campfire" (high freq)</p>
    </section>

    <hr>

    <!-- Part B: Flow Matching from Scratch -->
    <section class="section">
        <h2>Part B: Flow Matching from Scratch</h2>
        <p>
            In Part B, I trained my own flow matching model on MNIST from scratch, implementing UNet architectures 
            with time and class conditioning, and exploring iterative denoising for generative tasks.
        </p>
    </section>

    <hr>

    <!-- Part B 1: Training a Single-Step Denoising UNet -->
    <section id="partb1" class="section">
        <h2>Part B.1: Training a Single-Step Denoising UNet</h2>

        <h3>1.1 Implementing the UNet</h3>
        <p>
            I implemented a UNet architecture consisting of downsampling and upsampling blocks with skip connections. 
            The network uses the following building blocks:
        </p>
        <p>
            <strong>Conv:</strong> A standard convolutional block with Conv2d (kernel=3, stride=1, padding=1), 
            BatchNorm2d, and GELU activation. This maintains spatial resolution while changing channel dimension.
        </p>
        <p>
            <strong>DownConv:</strong> A downsampling block with Conv2d (kernel=3, stride=2, padding=1), 
            BatchNorm2d, and GELU. This reduces spatial resolution by half.
        </p>
        <p>
            <strong>UpConv:</strong> An upsampling block with ConvTranspose2d (kernel=4, stride=2, padding=1), 
            BatchNorm2d, and GELU. This doubles the spatial resolution.
        </p>
        <p>
            <strong>Flatten/Unflatten:</strong> Flatten uses view() to reshape the 7x7 feature map to a 1D vector. 
            Unflatten reshapes back to 7x7 spatial dimensions.
        </p>
        <p>
            The UNet encoder progressively downsamples the input while increasing channels, and the decoder 
            upsamples while using skip connections to preserve fine details from earlier layers.
        </p>

        <h3>1.2 Using the UNet to Train a Denoiser</h3>
        <p>
            The denoiser is trained using L2 loss to map noisy images \(z\) to clean images \(x\):
        </p>
        <div class="formula-box">
            \[ L = \mathbb{E}_{z,x} \|D_{\theta}(z) - x\|^2 \]
        </div>
        <p>
            Training data pairs are generated using the noising process:
        </p>
        <div class="formula-box">
            \[ z = x + \sigma \epsilon, \quad \text{where } \epsilon \sim \mathcal{N}(0, I) \]
        </div>

        <h4>Visualization of Noising Process</h4>
        <div class="image-container">
            <img src="code/noisy_visualization.png" alt="Noising visualization">
        </div>
        <p class="caption">Noising process with sigma = [0.0, 0.2, 0.4, 0.5, 0.6, 0.8, 1.0]</p>

        <h3>1.2.1 Training</h3>
        <p>
            I trained the denoiser with sigma = 0.5 for 5 epochs using Adam optimizer (lr=1e-4), batch size 256, and hidden dimension D=128.
        </p>

        <h4>Training Loss Curve</h4>
        <div class="image-container">
            <img src="code/training_curve_sigma0.5.png" style="width: 700px; height: auto;" alt="Training loss curve">
        </div>
        <p class="caption">Training loss curve for sigma = 0.5</p>

        <h4>Sample Results</h4>
        <div class="image-row">
            <img src="code/denoising_epoch1.png" alt="Epoch 1" style="height: 180px;">
            <img src="code/denoising_epoch5.png" alt="Epoch 5" style="height: 180px;">
        </div>
        <p class="caption">Epoch 1 -> Epoch 5 (top: clean, middle: noisy, bottom: denoised)</p>

        <h3>1.2.2 Out-of-Distribution Testing</h3>
        <p>
            I tested the denoiser (trained with sigma = 0.5) on different noise levels to evaluate its generalization.
        </p>

        <div class="image-container">
            <img src="code/ood_testing.png" style="height: 160px;" alt="OOD testing">
        </div>
        <p class="caption">OOD testing with sigma = [0.0, 0.2, 0.4, 0.5, 0.6, 0.8, 1.0] (top: clean, middle: noisy, bottom: denoised)</p>

        <h3>1.2.3 Denoising Pure Noise</h3>
        <p>
            To make denoising a generative task, I trained a model to denoise pure Gaussian noise.
        </p>

        <h4>Training Loss Curve</h4>
        <div class="image-container">
            <img src="code/training_curve_pure_noise.png" style="width: 700px; height: auto;" alt="Training loss curve for pure noise">
        </div>
        <p class="caption">Training loss curve for pure noise denoising</p>

        <h4>Sample Results After Epoch 1</h4>
        <div class="image-container">
            <img src="code/pure_noise_epoch1.png" alt="Pure noise epoch 1">
        </div>
        <p class="caption">Pure noise denoising results after epoch 1 (top: noise input, bottom: output)</p>

        <h4>Sample Results After Epoch 5</h4>
        <div class="image-container">
            <img src="code/pure_noise_epoch5.png" alt="Pure noise epoch 5">
        </div>
        <p class="caption">Pure noise denoising results after epoch 5 (top: noise input, bottom: output)</p>

        <p>
            <strong>Patterns Observed:</strong> The generated outputs appear as blurry, averaged versions of the training digits. 
            This happens because with MSE loss, the model learns to predict the point that minimizes the sum of squared distances 
            to all training examples - essentially the centroid of the training distribution. Since the input is pure noise with 
            no information about which digit to generate, the model outputs an average of all possible digits, resulting in 
            blurry, digit-like but non-specific patterns. This demonstrates why single-step denoising is insufficient for 
            high-quality generation - we need iterative denoising to progressively refine the output.
        </p>
    </section>

    <hr>

    <!-- Part B 2: Training a Flow Matching Model -->
    <section id="partb2" class="section">
        <h2>Part B.2: Training a Flow Matching Model</h2>

        <p>
            For iterative denoising, I implemented flow matching which learns to predict the flow from noisy to clean data. 
            The intermediate noisy samples are constructed via linear interpolation:
        </p>
        <div class="formula-box">
            \[ x_t = (1-t)x_0 + tx_1 \quad \text{where } x_0 \sim \mathcal{N}(0, 1), t \in [0, 1] \]
        </div>
        <p>
            The flow (velocity) is the derivative with respect to time:
        </p>
        <div class="formula-box">
            \[ u(x_t, t) = \frac{d}{dt} x_t = x_1 - x_0 \]
        </div>
        <p>
            The learning objective is:
        </p>
        <div class="formula-box">
            \[ L = \mathbb{E}_{x_0, x_1, t} \|(x_1 - x_0) - u_\theta(x_t, t)\|^2 \]
        </div>

        <h3>2.1 Adding Time Conditioning to UNet</h3>
        <p>
            I injected the time conditioning signal \(t\) into the UNet using FCBlocks. The conditioning modulates 
            the unflatten and up1 layers via multiplication:
        </p>
        <div class="formula-box">
            \[ \text{unflatten} = \text{unflatten} \times t_1, \quad \text{up1} = \text{up1} \times t_2 \]
        </div>

        <h3>2.2 Training the Time-Conditioned UNet</h3>
        <p>
            I trained the time-conditioned UNet for 10 epochs with batch size 64, hidden dimension D=64, 
            initial learning rate 1e-2, and exponential learning rate decay.
        </p>

        <h4>Training Loss Curve</h4>
        <div class="image-container">
            <img src="code/training_curve_time.png" style="width: 700px; height: auto;" alt="Time-conditioned training loss">
        </div>
        <p class="caption">Training loss curve for time-conditioned UNet</p>

        <h3>2.3 Sampling from the Time-Conditioned UNet</h3>
        <div class="image-row">
            <img src="code/time_samples_epoch1.png" alt="Epoch 1" style="height: 200px;">
            <img src="code/time_samples_epoch5.png" alt="Epoch 5" style="height: 200px;">
            <img src="code/time_samples_epoch10.png" alt="Epoch 10" style="height: 200px;">
        </div>
        <p class="caption">Epoch 1 -> Epoch 5 -> Epoch 10</p>
        <h3>2.4 Adding Class-Conditioning to UNet</h3>
        <p>
            To gain control over which digit to generate, I added class conditioning using one-hot vectors. 
            The conditioning is applied with dropout (p_uncond = 0.1) to enable classifier-free guidance:
        </p>
        <div class="formula-box">
            \[ \text{unflatten} = c_1 \times \text{unflatten} + t_1, \quad \text{up1} = c_2 \times \text{up1} + t_2 \]
        </div>

        <h3>2.5 Training the Class-Conditioned UNet</h3>

        <h4>Training Loss Curve</h4>
        <div class="image-container">
            <img src="code/training_curve_class.png" style="width: 700px; height: auto;" alt="Class-conditioned training loss">
        </div>
        <p class="caption">Training loss curve for class-conditioned UNet</p>

        <h3>2.6 Sampling from the Class-Conditioned UNet</h3>
        <p>
            I used classifier-free guidance with gamma = 5.0 to generate 4 instances of each digit (0-9).
        </p>

        </p>
        <div class="image-row">
            <img src="code/class_samples_Epoch_1.png" alt="Epoch 1" style="height: 120px;">
            <img src="code/class_samples_Epoch_5.png" alt="Epoch 5" style="height: 120px;">
            <img src="code/class_samples_Epoch_10.png" alt="Epoch 10" style="height: 120px;">
        </div>
        <p class="caption">Epoch 1 -> Epoch 5 -> Epoch 10 (gamma = 5.0)</p>

        <h3>Training Without Learning Rate Scheduler</h3>
        <p>
            To remove the exponential learning rate scheduler while maintaining performance, I used a constant 
            lower learning rate of 1e-3 (instead of starting at 1e-2 and decaying). This provides stable 
            training without the complexity of a scheduler.
        </p>

        <h4>Training Loss Curve (No Scheduler)</h4>
        <div class="image-container">
            <img src="code/training_curve_no_sched.png" style="width: 700px; height: auto;" alt="No scheduler training loss">
        </div>
        <p class="caption">Training loss curve without learning rate scheduler (lr = 1e-3)</p>

        <div class="image-row">
            <img src="code/class_samples_No_Scheduler_Epoch_1.png" alt="Epoch 1" style="height: 120px;">
            <img src="code/class_samples_No_Scheduler_Epoch_5.png" alt="Epoch 5" style="height: 120px;">
            <img src="code/class_samples_No_Scheduler_Epoch_10.png" alt="Epoch 10" style="height: 120px;">
        </div>
        <p class="caption">Epoch 1 -> Epoch 5 -> Epoch 10 (no scheduler, lr = 1e-3)</p>

        <p>
            By using a constant learning rate of 1e-3, the model achieves comparable results to the version with 
            the exponential scheduler. The key insight is that the scheduler's main effect is to reduce the 
            learning rate over time, which can be approximated by simply using a smaller constant learning rate 
            from the start.
        </p>
    </section>

    <hr>

    <footer>
        <p>CS180 Project 5 - Chunrui Huang - Fall 2025</p>
    </footer>

</body>
</html>
