<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>CS180 Project 4: Neural Radiance Fields</title>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            background-color: #f5f5f5;
            line-height: 1.6;
        }
        h1 {
            color: #2c3e50;
            border-bottom: 3px solid #3498db;
            padding-bottom: 10px;
        }
        h2 {
            color: #34495e;
            border-bottom: 2px solid #95a5a6;
            padding-bottom: 8px;
            margin-top: 40px;
        }
        h3 {
            color: #555;
            margin-top: 30px;
        }
        .section {
            background-color: white;
            padding: 30px;
            margin-bottom: 25px;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        .image-container {
            margin: 20px 0;
            text-align: center;
        }
        .image-container img {
            max-width: 800px;
            width: 100%;
            height: auto;
            border: 1px solid #ddd;
            border-radius: 4px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.15);
        }
        .image-grid-2 {
            display: grid;
            grid-template-columns: repeat(2, 1fr);
            gap: 20px;
            margin: 20px 0;
            max-width: 1200px;
            margin-left: auto;
            margin-right: auto;
        }
        .image-grid-2 img {
            width: 100%;
            height: auto;
            border: 1px solid #ddd;
            border-radius: 4px;
        }
        .caption {
            font-style: italic;
            color: #666;
            margin-top: 8px;
            font-size: 0.9em;
        }
        code {
            background-color: #f4f4f4;
            padding: 2px 6px;
            border-radius: 3px;
            font-family: 'Courier New', monospace;
            font-size: 0.9em;
        }
        .code-block {
            background-color: #f8f9fa;
            padding: 12px;
            border-left: 4px solid #3498db;
            margin: 15px 0;
            font-family: 'Courier New', monospace;
            font-size: 0.9em;
        }
        ul {
            margin: 12px 0;
            padding-left: 30px;
        }
        li {
            margin: 6px 0;
        }
        .note-box {
            background-color: #fff9e6;
            border-left: 4px solid #f39c12;
            padding: 15px;
            margin: 20px 0;
        }
    </style>
</head>
<body>
    <h1>CS180 Project 4: Neural Radiance Fields (NeRF)</h1>
    <p style="text-align: center; color: #666; margin-bottom: 30px;">Chunrui Huang | Fall 2025</p>
    
    <!-- PART 0 -->
    <div class="section">
        <h2>Part 0: Camera Calibration and Dataset Creation</h2>
        
        <h3>Part 0.1 & 0.2: Calibration and Data Capture</h3>
        
        <p>I calibrated my phone camera using 30-50 images of ArUco calibration grids (3×2 tags, 60mm×60mm each, 90mm horizontal spacing, 75.67mm vertical spacing). Used <code>cv2.calibrateCamera()</code> to compute intrinsics and distortion coefficients.</p>
        
        <p>Then captured 57 images of my object with a single 100mm×100mm ArUco tag, varying angles while maintaining consistent distance.</p>
        
        <div class="image-grid-2">
            <div class="image-container">
                <img src="code/calibration_images/IMG_7511.jpeg" alt="Calibration grid">
                <div class="caption">Sample calibration image with ArUco grid (6 tags visible)</div>
            </div>
            <div class="image-container">
                <img src="code/cali_toy/IMG_7646.jpeg" alt="Object scan">
                <div class="caption">Sample object scan with single ArUco tag</div>
            </div>
        </div>
        
        <h3>Part 0.3: Camera Pose Estimation</h3>
        
        <p>For each image, I used <code>cv2.solvePnP()</code> with detected ArUco corners to estimate camera pose. Inverted the world-to-camera transformation to get c2w matrices for NeRF training.</p>
        
        <div class="image-grid-2">
            <div class="image-container">
                <img src="code/view4.png" alt="Camera poses 1">
                <div class="caption">Viser visualization - camera frustums form dome pattern around object</div>
            </div>
            <div class="image-container">
                <img src="code/view5.png" alt="Camera poses 2">
                <div class="caption">Alternative view showing spatial distribution of camera poses</div>
            </div>
        </div>
        
        <h3>Part 0.4: Undistortion and Dataset Creation</h3>
        
        <p>Applied <code>cv2.undistort()</code> to remove lens distortion. Used <code>cv2.getOptimalNewCameraMatrix()</code> with <code>alpha=0</code> to crop black boundaries, then adjusted principal point for the crop offset. Split into 70% train, 15% val, 15% test and saved as <code>my_data.npz</code>.</p>
        
        <div class="image-container">
            <img src="code/boundary_check.png" alt="Undistortion comparison">
            <div class="caption">Undistortion: Original → Undistorted (black borders) → Cropped (final)</div>
        </div>
    </div>
    
    <!-- PART 1 -->
    <div class="section">
        <h2>Part 1: Fit a Neural Field to a 2D Image</h2>
        
        <h3>Architecture</h3>
        
        <p>Implemented MLP with sinusoidal positional encoding mapping (u,v) → (r,g,b). PE expands 2D input to 2*(2*L+1) dimensions.</p>
        
        <div class="code-block">
class NeuralField2D(nn.Module):<br>
&nbsp;&nbsp;&nbsp;&nbsp;def __init__(self, L=10, hidden_dim=256, num_layers=3):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;input_dim = 2 + 2 * 2 * L  # = 42 for L=10<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;layers = [nn.Linear(input_dim, hidden_dim), nn.ReLU()]<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for _ in range(num_layers - 1):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;layers += [nn.Linear(hidden_dim, hidden_dim), nn.ReLU()]<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;layers += [nn.Linear(hidden_dim, 3), nn.Sigmoid()]<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.network = nn.Sequential(*layers)<br><br>
Training: Adam lr=1e-2, 10k pixels/iter, 2000 iterations
        </div>
        
        <div class="image-container">
            <img src="code/part1architecture.png" alt="Part 1 Architecture">
            <div class="caption">Neural field architecture: Positional encoding → 3 hidden layers (256 width) → RGB output</div>
        </div>
        
        <h3>Results</h3>
        
        <div class="image-container">
            <img src="code/results/1_fox_training_progression.png" alt="Fox progression">
            <div class="caption">Fox training progression (0, 50, 150, 500, 2000 iters). Coarse structure emerges quickly, fine details refine gradually. Final PSNR: ~33 dB.</div>
        </div>
        
        <div class="image-container">
            <img src="code/results/2_cat_training_progression.png" alt="Cat progression">
            <div class="caption">Cat training progression (my image). Final PSNR: ~30 dB.</div>
        </div>
        
        <div class="image-container">
            <img src="code/results/fox_psnr_training.png" alt="Fox PSNR curve">
            <div class="caption">PSNR training curve for fox image. Rapid improvement in first 500 iterations, then gradual refinement reaching ~33 dB.</div>
        </div>
        
        <h3>Hyperparameter Study</h3>
        
        <div class="image-container">
            <img src="code/results/3_hyperparameter_grid_2x2.png" alt="Hyperparameter grid">
            <div class="caption">L × Width grid: (L=2,W=64) blurry, (L=2,W=256) smooth, (L=10,W=64) limited detail, (L=10,W=256) best quality. Higher L captures high-frequency details, wider network increases capacity.</div>
        </div>
    </div>
    
    <!-- PART 2 -->
    <div class="section">
        <h2>Part 2: Neural Radiance Field (NeRF)</h2>
        
        <h3>Implementation Overview</h3>
        
        <p><strong>Part 2.1 - Create Rays:</strong> Implemented <code>transform(c2w, x_c)</code>, <code>pixel_to_camera(K, uv, s)</code>, and <code>pixel_to_ray(K, c2w, uv)</code> to convert pixels to 3D rays with origins and normalized directions.</p>
        
        <p><strong>Part 2.2 - Sampling:</strong> Sample 64 points per ray between near/far bounds with random perturbation during training to prevent overfitting.</p>
        
        <p><strong>Part 2.3 - Dataset:</strong> Pre-computed ~4M rays from 100 training images for efficient batch sampling.</p>
        
        <p><strong>Part 2.4 - NeRF Network:</strong> MLP with position encoding (L=10) and direction encoding (L=4). Outputs density from position, RGB from position+direction.</p>
        
        <p><strong>Part 2.5 - Volume Rendering:</strong> Discrete volume rendering using alpha compositing with transmittance.</p>
        
        <h3>Dataset</h3>
        <p>Lego (200×200): 100 train, 10 val, 60 test poses. Focal=138.89.</p>
        
        <div class="image-container">
            <img src="code/part2/01_dataset_samples.png" alt="Dataset samples">
            <div class="caption">Sample training images from multiple viewpoints</div>
        </div>
        
        <h3>Part 2.1: Ray Generation</h3>
        
        <p>Implemented coordinate transformations:</p>
        <ul>
            <li><code>transform(c2w, x_c)</code>: Camera to world</li>
            <li><code>pixel_to_camera(K, uv, s)</code>: Pixel to camera coordinates</li>
            <li><code>pixel_to_ray(K, c2w, uv)</code>: Generate ray origin and normalized direction</li>
        </ul>
        
        <div class="image-container">
            <img src="code/part2/02_ray_directions.png" alt="Ray directions">
            <div class="caption">Ray directions visualization. Each pixel maps to a ray from camera center, colored by XYZ direction.</div>
        </div>
        
        <h3>Part 2.2 & 2.3: Sampling and Dataset</h3>
        
        <p>Sampled 64 points per ray between near=2.0 and far=6.0 with random perturbation during training. Pre-computed ~4M rays (100 images × 200×200) for efficient batch sampling.</p>
        
        <div class="image-grid-2">
            <div class="image-container">
                <img src="code/part2/2.3viser_one_cam.png" alt="Viser one camera">
                <div class="caption">Viser visualization: Rays from a single camera with sampled points</div>
            </div>
            <div class="image-container">
                <img src="code/part2/2.3visere_random_cam.png" alt="Viser random cameras">
                <div class="caption">Viser visualization: 100 randomly sampled rays from multiple cameras</div>
            </div>
        </div>
        
        <h3>Part 2.4 & 2.5: NeRF Architecture and Volume Rendering</h3>
        
        <div class="code-block">
<strong>Position Encoding:</strong> L_pos=10 → 3*(2*10+1) = 63 dimensions<br>
<strong>Direction Encoding:</strong> L_dir=4 → 3*(2*4+1) = 27 dimensions<br><br>
<strong>Network Architecture:</strong><br>
Position → Density (σ) + [Position+Direction] → Color (RGB)
        </div>
        
        <p><strong>Volume Rendering Equation:</strong></p>
        <div style="background-color: #f8f9fa; padding: 25px; margin: 20px 0; border-left: 4px solid #3498db; border-radius: 4px;">
            <p style="text-align: center; font-size: 1.1em;">
                $$\hat{C}(\mathbf{r})=\sum_{i=1}^N T_i\left(1-\exp \left(-\sigma_i \delta_i\right)\right) \mathbf{c}_i, \text{ where } T_i=\exp \left(-\sum_{j=1}^{i-1} \sigma_j \delta_j\right)$$
            </p>
            <p style="margin-top: 20px; font-size: 0.95em;">
                where \(\mathbf{c}_i\) is the RGB color at sample \(i\), \(T_i\) is the transmittance (probability of ray reaching sample \(i\)), and \(1 - \exp(-\sigma_i \delta_i)\) is the alpha (probability of ray terminating at sample \(i\)).
            </p>
        </div>
        
        <p><strong>Training:</strong> Adam lr=5e-4, 10k rays/iter, 2000 iterations</p>
        
        <h3>Training Results</h3>
        
        <div class="image-container">
            <img src="code/part2/04_training_curves.png" alt="Training curves">
            <div class="caption">Training curves. Left: MSE loss decreases steadily. Right: PSNR curves - validation reaches ~24 dB, exceeding 23 dB requirement ✓</div>
        </div>
        
        <div class="image-container">
            <img src="code/part2/05_intermediate_renders.png" alt="Intermediate renders">
            <div class="caption">Training progression (1-2000 iters). Shape emerges by iter 200, details refine through iter 2000.</div>
        </div>
        
        <div class="image-container">
            <img src="code/part2/06_validation_comparison.png" alt="Validation">
            <div class="caption">Validation: Ground truth (left) vs predictions (right). PSNR 23-26 dB across views.</div>
        </div>
        
        <h3>Novel View Synthesis</h3>
        
        <div class="image-container">
            <img src="code/part2/07_novel_views.gif" alt="Novel views" style="max-width: 320px;">
            <div class="caption">360° novel view synthesis (60 test poses). Smooth, consistent geometry and appearance.</div>
        </div>
        
        <div class="image-container">
            <img src="code/part2/08_video_samples.png" alt="Video samples">
            <div class="caption">Sample frames showing temporal consistency</div>
        </div>
    </div>
    
    <!-- PART 2.6 -->
    <div class="section">
        <h2>Part 2.6: NeRF on My Own Data</h2>
        
        <h3>Key Changes</h3>
        
        <p><strong>Scene bounds:</strong> Changed near=2.0, far=6.0 to near=0.02, far=0.5 (object captured at close range)</p>
        
        <p><strong>Training config:</strong> Batch size 4096 (fewer images), 5000 iterations (real-world complexity), lr=5e-4</p>
        
        <p><strong>Memory:</strong> Implemented lazy ray loading instead of pre-computing all rays (57 images × H×W)</p>
        
        <h3>Results</h3>
        
        <div class="image-container">
            <img src="code/part2_6/loss.png" alt="Training loss">
            <div class="caption">Training loss over 5000 iterations - steady decrease despite real-world challenges</div>
        </div>
        
        <div class="image-container">
            <img src="code/part2_6/intermediate.png" alt="Intermediate renders">
            <div class="caption">Training progression (0, 500, 1000, 2000, 3000, 4500). ArUco tag emerges first, object follows.</div>
        </div>
        
        <div class="image-container">
            <img src="code/part2_6/novel_views_30.gif?v=1" alt="Novel views" style="max-width: 320px;">
            <div class="caption">360° novel view synthesis (30 frames). Successfully synthesizes unseen viewpoints.</div>
        </div>
        
        <h3>Discussion</h3>
        
        <p><strong>Code/Hyperparameter Changes from Part 2 (Lego):</strong></p>
        
        <p><strong>1. Dataset Loading:</strong></p>
        <ul>
            <li>Changed from <code>lego_200x200.npz</code> to <code>my_data.npz</code></li>
            <li>Captured 57 images, 39 used for training (after 70-15-15 split) vs Lego's 100</li>
        </ul>
        
        <p><strong>2. Scene Scale Adjustment (Critical):</strong></p>
        <ul>
            <li><code>NEAR = 0.02</code> (changed from 2.0), <code>FAR = 0.5</code> (changed from 6.0)</li>
            <li><strong>Reason:</strong> Object captured at much closer range than Lego scene</li>
            <li>Without this, all sample points would be outside the actual scene</li>
        </ul>
        
        <p><strong>3. Training Configuration:</strong></p>
        <ul>
            <li><code>BATCH_SIZE = 4096</code> (reduced from 10000) - fewer images require smaller batch to prevent overfitting</li>
            <li><code>N_ITERS = 5000</code> (increased from 2000) - real-world complexity needs more iterations</li>
        </ul>
        
        <p><strong>4. Memory Optimization:</strong></p>
        <ul>
            <li>Implemented <code>RaysDatasetLazy</code> instead of precomputing all rays</li>
            <li><strong>Reason:</strong> RAM limitations - precomputing 39 images × (H×W) rays caused crash</li>
            <li><strong>Trade-off:</strong> Slightly slower but necessary for execution</li>
        </ul>
        
        <p><strong>5. Volume Rendering Function:</strong></p>
        <ul>
            <li>Rewrote <code>volume_render()</code> to handle batch dimensions correctly</li>
            <li>Fixed tensor shape mismatches between sigma and delta calculations</li>
        </ul>
        
        <p><strong>6. Novel View Camera Path:</strong></p>
        <ul>
            <li><code>START_POS = c2ws_train[0][:3, 3]</code> (based on actual training camera)</li>
            <li><code>NUM_SAMPLES = 30</code> frames (reduced from 60 for faster rendering)</li>
        </ul>
        
        <p><strong>Observations:</strong></p>
        <ul>
            <li><strong>Training Behavior:</strong> Convergence slower than Lego due to real-world complexity, background clutter, and lighting variations</li>
            <li><strong>ArUco tag learned first</strong> (high contrast), then surrounding context</li>
            <li><strong>Results Quality:</strong> Recognizable object with some blur/artifacts (expected with limited training images)</li>
            <li>Quality heavily dependent on Part 0 calibration accuracy</li>
        </ul>
        
        <p><strong>Challenges Encountered:</strong></p>
        <ol>
            <li><strong>RAM constraints</strong> → Solved with lazy loading</li>
            <li><strong>Scene scale mismatch</strong> → Solved by adjusting near/far bounds</li>
            <li><strong>Long rendering times</strong> → Mitigated by reducing frame count</li>
            <li><strong>Tensor dimension errors</strong> → Fixed volume rendering implementation</li>
        </ol>
    </div>
    
</body>
</html>
